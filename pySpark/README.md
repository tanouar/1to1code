# PySpark Utils

BibliothÃ¨que d'utilitaires pour Apache Spark / PySpark dÃ©veloppÃ©e pour faciliter le traitement distribuÃ© de donnÃ©es et d'images.

## ğŸ¯ FonctionnalitÃ©s

- **Monitoring** : Suivi des jobs Spark avec historique et visualisations
- **Traitement d'images** : Chargement, transformation et extraction de features
- **Gestion de donnÃ©es** : Optimisation Parquet, partitionnement intelligent
- **Helpers** : Utilitaires pour DataFrames, performance, validation
- **Configuration** : SparkSession builder avec configs prÃ©dÃ©finies

## ğŸ“¦ Installation

### Via GitHub
```bash
pip install git+https://github.com/tanouar/1to1code.git#subdirectory=pySpark
```

### Via clone local
```bash
git clone https://github.com/tanouar/1to1code.git
cd 1to1code/pySpark
pip install -e .
```

### Dans Google Colab
```python
!git clone https://github.com/tanouar/1to1code.git
!pip install ./1to1code/pySpark --break-system-packages -q
```

## ğŸš€ Utilisation Rapide

### Monitoring des Jobs
```python
from pyspark_utils.monitoring import SparkJobMonitor

monitor = SparkJobMonitor(spark)
result = monitor.execute_and_monitor(
    lambda: df.count(), 
    "Mon premier job"
)
monitor.show_history()
monitor.show_chart()
```

### Traitement d'Images
```python
from pyspark_utils.images import ImageLoader, ImageTransformer

# Chargement
loader = ImageLoader(spark)
df_images = loader.load_images("/path/to/images", with_metadata=True)

# Transformation
transformer = ImageTransformer()
df_resized = transformer.resize(df_images, width=224, height=224)
df_features = transformer.extract_color_features(df_resized)
```

### Gestion de DonnÃ©es
```python
from pyspark_utils.data import ParquetLoader, PartitioningHelper

# Chargement optimisÃ©
loader = ParquetLoader(spark)
df = loader.load("/path/to/data.parquet")

# Suggestions de partitionnement
helper = PartitioningHelper()
suggestions = helper.suggest_partitions(df, ["date", "region"])
```

### Helpers DataFrame
```python
from pyspark_utils.helpers import DataFrameHelper

helper = DataFrameHelper()
helper.show_info(df, name="Mon DataFrame")
helper.get_partition_info(df)
```

## ğŸ“š Documentation

Voir le dossier [docs/](docs/) pour la documentation complÃ¨te :

- [Guide de Monitoring](docs/monitoring.md)
- [Guide Traitement d'Images](docs/images.md)
- [Guide Gestion de DonnÃ©es](docs/data.md)
- [RÃ©fÃ©rence API](docs/api_reference.md)

## ğŸ“ Structure du Projet

```
pyspark-utils/
â”œâ”€â”€ pyspark_utils/          # Package principal
â”‚   â”œâ”€â”€ monitoring/         # Monitoring Spark
â”‚   â”œâ”€â”€ images/            # Traitement d'images
â”‚   â”œâ”€â”€ data/              # Gestion de donnÃ©es
â”‚   â”œâ”€â”€ helpers/           # Helpers gÃ©nÃ©riques
â”‚   â”œâ”€â”€ config/            # Configuration Spark
â”‚   â””â”€â”€ utils/             # Utilitaires
â”œâ”€â”€ examples/              # Exemples d'utilisation
â”œâ”€â”€ docs/                  # Documentation
â””â”€â”€ tests/                 # Tests unitaires (Ã  venir)
```

## ğŸ“ Exemples

Consultez le dossier [examples/](examples/) pour des exemples complets :

- `example_monitoring.py` : Monitoring de jobs
- `example_images.py` : Traitement d'images
- `example_partitioning.py` : Partitionnement de donnÃ©es
- `notebook_1_parquet.ipynb` : Notebook complet sur Parquet
- `notebook_2_images.ipynb` : Notebook complet sur les images

## ğŸ¤ Contribution

Les contributions sont bienvenues ! N'hÃ©sitez pas Ã  ouvrir des issues ou des pull requests.

## ğŸ“„ Licence

MIT License - Voir [LICENSE](LICENSE)

## âœ¨ Auteur

DÃ©veloppÃ© dans le cadre d'un projet d'apprentissage PySpark
